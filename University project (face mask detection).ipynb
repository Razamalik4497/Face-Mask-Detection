{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southwest-difficulty",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES AND MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handled-updating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait... \n",
      " ----- Model and Libraries are importing---- \n"
     ]
    }
   ],
   "source": [
    "### IMPORT PACKAGES\n",
    "\n",
    "print ( \"Please wait... \\n ----- Model and Libraries are importing---- \")\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "\n",
    "###  LOADING TRAINED MODEL \n",
    "model = load_model('inceptionV3-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thrown-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define height and width of image which pass through the model \n",
    "# Load the Cascade face Classifier\n",
    "\n",
    "img_width, img_hight = 100, 100\n",
    "face_cascade = cv2.CascadeClassifier(\"D:/Uni Project/live mask detection app/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "### make function for image detection\n",
    "\n",
    "def action1():\n",
    "  \n",
    "    # Define parameter for text styling\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "    org = (1, 1)\n",
    "    class_lable=' '      \n",
    "    fontScale = 1\n",
    "    color = (255, 0, 0) \n",
    "    thickness = 2\n",
    "    img_count=1\n",
    "\n",
    "    # Importe Images from the path \n",
    "    # images Converte into grayscale\n",
    "    # Detect the faces with Cascade detectMultiScale\n",
    "\n",
    "    image_path = glob.glob(\"D:/Uni Project/live mask detection app/images/*.jpg\")\n",
    "   \n",
    "    for image in image_path:\n",
    "        color_img=cv2.imread(image)\n",
    "        gray_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_img, 1.1, 6) \n",
    "\n",
    "        # (x=axis, y=axis , w=width, h=height) find four point to make rectangle on face \n",
    "        # display color images after detecting face area, which is passed throuhg gray images\n",
    "        # images save in the directory \n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            org = (x-10,y-10) \n",
    "            print(img_count)\n",
    "            color_face = color_img[y:y+h,x:x+w] \n",
    "            cv2.imwrite('D:/Uni Project/detected images/images from images/input/%dface.jpg'%(img_count),color_face)\n",
    "            img = load_img('D:/Uni Project/detected images/images from images/input/%dface.jpg'%(img_count), target_size=(img_width,img_hight))\n",
    "            img_count+=1\n",
    "\n",
    "            # Convert image into array for the purpose of prediction  \n",
    "            \n",
    "            img = img_to_array(img)/255\n",
    "            img = np.expand_dims(img,axis=0)\n",
    "            pred_prob = model.predict(img)\n",
    "            pred=np.argmax(pred_prob)\n",
    "\n",
    "\n",
    "            # 'without mask' images save in appropriate folder\n",
    "            # 'with mask' images save in appropriate folder\n",
    "            \n",
    "            if pred==0:\n",
    "                print(\"User without mask - predic = \",pred_prob[0][0])\n",
    "                class_lable = \"No Mask\"\n",
    "                color = (255, 0, 0)\n",
    "                cv2.imwrite('D:Uni Project/detected images/images from images/without mask/%dface.jpg'%(img_count),color_face)\n",
    "                cv2.rectangle(color_img, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "                # Using cv2.putText() method \n",
    "                cv2.putText(color_img, class_lable, org, font,fontScale, color, thickness, cv2.LINE_AA)            \n",
    "            else:\n",
    "                print('user with mask - prob = ',pred_prob[0][1])\n",
    "                class_lable = \"Mask\"\n",
    "                color = (0, 255, 0)\n",
    "                cv2.imwrite('D:/Uni Project/detected images/images from images/with mask/%dface.jpg'%(img_count),color_face)\n",
    "                cv2.rectangle(color_img, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "                # Using cv2.putText() method \n",
    "                cv2.putText(color_img, class_lable, org, font,fontScale, color, thickness, cv2.LINE_AA) \n",
    "                \n",
    "# make function for image detection through webcam  \n",
    "\n",
    "def action2():\n",
    "    \n",
    "    # capture images from webcam\n",
    "    # Define parameter for text styling\n",
    "\n",
    "    cap = cv2.VideoCapture(0) \n",
    "    img_count_full = 0\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "    org = (1, 1)\n",
    "    class_lable=' '      \n",
    "    fontScale = 1\n",
    "    color = (255, 0, 0) \n",
    "    thickness = 2\n",
    "    \n",
    "    \n",
    "    while(True):\n",
    "        img_count_full += 1\n",
    "\n",
    "        #read image from webcam\n",
    "        # Convert to grayscale\n",
    "        # Detect the faces with Cascade detectMultiScale\n",
    "        \n",
    "        responce, color_img = cap.read()\n",
    "        gray_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_img, 1.1, 6)\n",
    "\n",
    "        # (x=axis, y=axis , w=width, h=height) find four point to make rectangle on face \n",
    "        # display color images after detecting face area, which is passed throuhg gray images\n",
    "        # images save in the directory \n",
    "        \n",
    "        img_count = 0\n",
    "        for (x, y, w, h) in faces:\n",
    "            org = (x-10,y-10)\n",
    "            img_count +=1 \n",
    "            color_face = color_img[y:y+h,x:x+w] \n",
    "            cv2.imwrite('D:/Uni Project/detected images/images from live stream/input/%d%dface.jpg'%(img_count_full,img_count),color_face)\n",
    "            img = load_img('D:/Uni Project/detected images/images from live stream/input/%d%dface.jpg'%(img_count_full,img_count), target_size=(img_width,img_hight))\n",
    "\n",
    "            img = img_to_array(img)/255\n",
    "            img = np.expand_dims(img,axis=0)\n",
    "            pred_prob = model.predict(img)\n",
    "            #print(pred_prob[0][0].round(2))\n",
    "            pred=np.argmax(pred_prob)\n",
    "\n",
    "            # 'without mask' images save in appropriate folder\n",
    "            # 'with mask' images save in appropriate folder\n",
    "            \n",
    "            if pred==0:\n",
    "                print(\"User without mask - predic = \",pred_prob[0][0])\n",
    "                class_lable = \"No Mask\"\n",
    "                color = (255, 0, 0)\n",
    "                cv2.imwrite('D:/Uni Project/detected images/images from live stream/without mask/%d%dface.jpg'%(img_count_full,img_count),color_face)\n",
    "            else:\n",
    "                print('user with mask - prob = ',pred_prob[0][1])\n",
    "                class_lable = \"Mask\"\n",
    "                color = (0, 255, 0)\n",
    "                cv2.imwrite('D:/Uni Project/detected images/images from live stream/with mask/%d%dface.jpg'%(img_count_full,img_count),color_face)\n",
    "\n",
    "            # Opencv method rectangle() & putText()\n",
    "            # all text parameter is defined above  \n",
    "            \n",
    "            cv2.rectangle(color_img, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "            cv2.putText(color_img, class_lable, org, font,  \n",
    "            fontScale, color, thickness, cv2.LINE_AA) \n",
    "\n",
    "        # Display the image window \n",
    "            \n",
    "        cv2.imshow('LIVE face mask detection', color_img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the VideoCapture object\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# make function for image detection through Recorded video\n",
    "def action3():\n",
    "    \n",
    "    # path of video \n",
    "    # Define parameter for text styling\n",
    "    \n",
    "    cap = cv2.VideoCapture('D:/Uni Project/live mask detection app/videos/my_video.mp4') # for video\n",
    "    img_count_full = 0\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "    org = (1, 1)\n",
    "    class_lable=' '      \n",
    "    fontScale = 1 \n",
    "    color = (255, 0, 0) \n",
    "    thickness = 2\n",
    "    \n",
    "    ## Start reading images and prediction\n",
    "    \n",
    "    while(True):\n",
    "        img_count_full += 1\n",
    "        \n",
    "        # read image from video\n",
    "        responce, color_img = cap.read()\n",
    "\n",
    "        #after complete the video it will terminate the reading \n",
    "        if responce == False:\n",
    "            break    \n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect the faces with Cascade detectMultiScale\n",
    "        faces = face_cascade.detectMultiScale(gray_img, 1.1, 6)\n",
    "\n",
    "        # (x=axis, y=axis , w=width, h=height) find four point to make rectangle on face \n",
    "        # display color images after detecting face area, which is passed throuhg gray images\n",
    "        # images save in the directory \n",
    "        img_count = 0\n",
    "        for (x, y, w, h) in faces:\n",
    "            org = (x-10,y-10)\n",
    "            img_count += 1 \n",
    "            color_face = color_img[y:y+h,x:x+w] \n",
    "            cv2.imwrite('D:/Uni Project/detected images/images from recorded video/input/%d%dface.jpg'%(img_count_full,img_count),color_face)\n",
    "            img = load_img('D:/Uni Project/detected images/images from recorded video/input/%d%dface.jpg'%(img_count_full,img_count), target_size=(img_width,img_hight))\n",
    "\n",
    "            # Rescale images for prediction purpose\n",
    "            \n",
    "            img = img_to_array(img)/255\n",
    "            img = np.expand_dims(img,axis=0)\n",
    "            pred_prob = model.predict(img)\n",
    "            pred=np.argmax(pred_prob)\n",
    "\n",
    "            # 'without mask' images save in appropriate folder\n",
    "            # 'with mask' images save in appropriate folder\n",
    "            \n",
    "            if pred==0:\n",
    "                print(\"User without mask - predic = \",pred_prob[0][0])\n",
    "                class_lable = \"No Mask\"\n",
    "                color = (255, 0, 0)\n",
    "                cv2.imwrite('D:/Uni Project/detected images/images from recorded video/without mask/%d%dface.jpg'%(img_count_full,img_count),color_face)\n",
    "            else:\n",
    "                print('user with mask - prob = ',pred_prob[0][1])\n",
    "                class_lable = \"Mask\"\n",
    "                color = (0, 255, 0)\n",
    "                cv2.imwrite('D:/Uni Project/detected images/images from recorded video/with mask/%d%dface.jpg'%(img_count_full,img_count),color_face)\n",
    "\n",
    "            # Opencv method rectangle() & putText()\n",
    "            # all text parameter is defined above  \n",
    "            \n",
    "            cv2.rectangle(color_img, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "            cv2.putText(color_img, class_lable, org, font, fontScale, color, thickness, cv2.LINE_AA) \n",
    "\n",
    "        # display image\n",
    "        cv2.imshow('LIVE face mask detection', color_img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the VideoCapture object\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "objective-receipt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ MODEL HAS BEEN LOADED ------- \n",
      "Please Select anyone option \n",
      " | 1 : Image Detection | \n",
      " | 2 : webcam detection  | \n",
      " | 3 : Recorded Frames |\n",
      "1\n",
      "| 1 : Image Detection |\n",
      "-----------------------\n",
      "  Under Proccessing\n",
      "-----------------------\n",
      "1\n",
      "user with mask - prob =  1.0\n",
      "2\n",
      "user with mask - prob =  0.9999975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Raza Pc\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3427, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-35f53aad6e4e>\", line 17, in <module>\n",
      "    print(action1())\n",
      "  File \"<ipython-input-2-bc0840294649>\", line 30, in action1\n",
      "    faces = face_cascade.detectMultiScale(gray_img, 1.1, 6)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Raza Pc\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2054, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Raza Pc\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Raza Pc\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raza Pc\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Raza Pc\\.conda\\envs\\tf\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Raza Pc\\.conda\\envs\\tf\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Raza Pc\\.conda\\envs\\tf\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Raza Pc\\.conda\\envs\\tf\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\Raza Pc\\.conda\\envs\\tf\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-35f53aad6e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mcompleted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-bc0840294649>\u001b[0m in \u001b[0;36maction1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mgray_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2053\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2054\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2055\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2054\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2057\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "    \n",
    "def processing():\n",
    "    print(\"-----------------------\")\n",
    "    print( \"  Under Proccessing\")   \n",
    "    print(\"-----------------------\")\n",
    "\n",
    "def completed():\n",
    "    print(\"-----------------------\")\n",
    "    print(\"job is completed\")\n",
    "    print(\"-----------------------\")    \n",
    "\n",
    "print(\" ------ MODEL HAS BEEN LOADED ------- \")\n",
    "user_input = input(\"Please Select anyone option \\n | 1 : Image Detection | \\n | 2 : webcam detection  | \\n | 3 : Recorded Frames |\\n\" )\n",
    "\n",
    "if user_input == '1':\n",
    "    print(\"| 1 : Image Detection |\")\n",
    "    processing()\n",
    "    print(action1())\n",
    "    completed()\n",
    "elif user_input == '2':\n",
    "    print(\"| 2 : webcam detection  |\")\n",
    "    processing()\n",
    "    print(action2())\n",
    "    completed()\n",
    "elif user_input == '3':\n",
    "    print(\"| 3 : Recorded Frames | \")\n",
    "    processing()\n",
    "    print(action3())\n",
    "    completed()\n",
    "else:\n",
    "    print(\"Invalid command \\n please Rerun the code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-contamination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
